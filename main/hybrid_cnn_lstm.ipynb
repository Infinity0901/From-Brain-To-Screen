{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "!pip install tensorflow-addons\n",
    "!pip install mne\n",
    "import tensorflow_addons as tfa\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, Dropout, SpatialDropout1D, SpatialDropout2D, BatchNormalization\n",
    "from tensorflow.keras.layers import Flatten, InputSpec, Layer, Concatenate, AveragePooling2D, MaxPooling2D, Reshape, Permute\n",
    "from tensorflow.keras.layers import Conv2D, SeparableConv2D, DepthwiseConv2D, LayerNormalization\n",
    "from tensorflow.keras.layers import TimeDistributed, Lambda, AveragePooling1D, Add, Conv1D, Multiply\n",
    "from tensorflow.keras.layers import Conv2D, DepthwiseConv2D, BatchNormalization, Activation, Concatenate, Input, AveragePooling2D, Dropout, Flatten, Dense, LSTM\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.constraints import max_norm, unit_norm \n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow_addons.layers import WeightNormalization\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "import random\n",
    "import mne\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.metrics import silhouette_score, confusion_matrix\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fs = 256             # Sampling frequency\n",
    "n_channels = 4       # Number of channels\n",
    "Wn = 1               # Sampling window duration\n",
    "n_samples = Wn*Fs    # sampling window length per channel\n",
    "\n",
    "n_ff = [2,4,8,16]    # Number of frequency filters for each inception module of EEG-ITNet\n",
    "n_sf = [1,1,1,1]     # Number of spatial filters in each frequency sub-band of EEG-ITNet\n",
    "batch_size = 32 \n",
    "epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Hybrid_CNN_LSTM(Chans, Samples, out_class=3, drop_rate=0.2):\n",
    "    Input_block = Input(shape=(Chans, Samples, 1))\n",
    "    \n",
    "    # CNN\n",
    "    block1 = Conv2D(32, (1, 16), activation='relu', padding='same', name='Conv1')(Input_block)\n",
    "    block1 = BatchNormalization()(block1)\n",
    "    block1 = DepthwiseConv2D((Chans, 1), activation='relu', depth_multiplier=1, padding='valid', name='DepthConv1')(block1)\n",
    "    block1 = BatchNormalization()(block1)\n",
    "    block1 = Activation('elu')(block1)\n",
    "\n",
    "    block2 = Conv2D(64, (1, 32), activation='relu', padding='same', name='Conv2')(Input_block)\n",
    "    block2 = BatchNormalization()(block2)\n",
    "    block2 = DepthwiseConv2D((Chans, 1), activation='relu', depth_multiplier=1, padding='valid', name='DepthConv2')(block2)\n",
    "    block2 = BatchNormalization()(block2)\n",
    "    block2 = Activation('elu')(block2)\n",
    "\n",
    "    block3 = Conv2D(128, (1, 64), activation='relu', padding='same', name='Conv3')(Input_block)\n",
    "    block3 = BatchNormalization()(block3)\n",
    "    block3 = DepthwiseConv2D((Chans, 1), activation='relu', depth_multiplier=1, padding='valid', name='DepthConv3')(block3)\n",
    "    block3 = BatchNormalization()(block3)\n",
    "    block3 = Activation('elu')(block3)\n",
    "\n",
    "    block4 = Conv2D(256, (1, 128), activation='relu', padding='same', name='Conv4')(Input_block)\n",
    "    block4 = BatchNormalization()(block4)\n",
    "    block4 = DepthwiseConv2D((Chans, 1), activation='relu', depth_multiplier=1, padding='valid', name='DepthConv4')(block4)\n",
    "    block4 = BatchNormalization()(block4)\n",
    "    block4 = Activation('elu')(block4)\n",
    "\n",
    "    # Concaténation des blocs CNN\n",
    "    block = Concatenate(axis=-1)([block1, block2, block3, block4])\n",
    "\n",
    "    # Réduction de dimension et préparation pour LSTM\n",
    "    lstm_input = AveragePooling2D((1, 4))(block)\n",
    "    lstm_input = Dropout(drop_rate)(lstm_input)\n",
    "    lstm_input = Flatten()(lstm_input)\n",
    "    lstm_input = Dense(128, activation='relu')(lstm_input)\n",
    "    lstm_input = Dropout(drop_rate)(lstm_input)\n",
    "    \n",
    "    # Ajout de LSTM\n",
    "    lstm_output = LSTM(64, return_sequences=False)(tf.expand_dims(lstm_input, axis=1))\n",
    "\n",
    "    # Partie classification\n",
    "    out = Dense(out_class, activation='softmax')(lstm_output)\n",
    "\n",
    "    # Création du modèle\n",
    "    model = Model(inputs=Input_block, outputs=out)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "files1 = glob(\"main/record/Train/*gauche*\")                                           \n",
    "files2 = glob(\"main/record/Train/*droite*\")                                           \n",
    "files3 = glob(\"main/record/Train/*neutre*\")\n",
    "\n",
    "\n",
    "dfs1 = []\n",
    "dfs2 = []\n",
    "dfs3 = []\n",
    "\n",
    "for f in files1:\n",
    "    df = pd.read_csv(f)\n",
    "    cols_remove = ['timestamps', 'Right AUX']\n",
    "    df = df.loc[:, ~df.columns.isin(cols_remove)]\n",
    "    df.columns = df.columns.str.replace('RAW_', '', 1)\n",
    "    df = df.fillna(df.mean())\n",
    "    dfs1.append(df)  \n",
    "\n",
    "data1 = pd.concat(dfs1, ignore_index=True)\n",
    "\n",
    "\n",
    "for f in files2:\n",
    "    df = pd.read_csv(f)\n",
    "    cols_remove = ['timestamps', 'Right AUX']\n",
    "    df = df.loc[:, ~df.columns.isin(cols_remove)]\n",
    "    df.columns = df.columns.str.replace('RAW_', '', 1)\n",
    "    df = df.fillna(df.mean())\n",
    "    dfs2.append(df)  \n",
    "\n",
    "data2 = pd.concat(dfs2, ignore_index=True)\n",
    "\n",
    "for f in files3:\n",
    "    df = pd.read_csv(f)\n",
    "    cols_remove = ['timestamps', 'Right AUX']\n",
    "    df = df.loc[:, ~df.columns.isin(cols_remove)]\n",
    "    df.columns = df.columns.str.replace('RAW_', '', 1)\n",
    "    df = df.fillna(df.mean())\n",
    "    dfs3.append(df)  \n",
    "\n",
    "data3 = pd.concat(dfs3, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data1)\n",
    "print(data1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNE Epochs Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertDF2MNE(sub):\n",
    "    info = mne.create_info(list(sub.columns), ch_types=['eeg'] * len(sub.columns), sfreq=256)\n",
    "    info.set_montage('standard_1020')\n",
    "    data=mne.io.RawArray(sub.T, info)\n",
    "    data.set_eeg_reference()\n",
    "    epochs=mne.make_fixed_length_epochs(data,duration=Wn,overlap=0.2*Wn)\n",
    "    return epochs.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_left = np.empty((0, n_channels, n_samples))\n",
    "y_left = np.empty(0)\n",
    "\n",
    "\n",
    "x_right = np.empty((0, n_channels, n_samples))\n",
    "y_right = np.empty(0)\n",
    "\n",
    "x_neutral = np.empty((0, n_channels, n_samples))\n",
    "y_neutral = np.empty(0)\n",
    "\n",
    "\n",
    "\n",
    "data = convertDF2MNE(data1)\n",
    "for i in enumerate(data):\n",
    "  label=0\n",
    "  y_left=np.append(y_left,label)\n",
    "x_left = np.append(x_left,data,axis=0)\n",
    "\n",
    "data = convertDF2MNE(data2)\n",
    "for i in enumerate(data):\n",
    "  label=1\n",
    "  y_right=np.append(y_right,label)\n",
    "x_right = np.append(x_right,data,axis=0)\n",
    "\n",
    "\n",
    "data = convertDF2MNE(data3)\n",
    "for i in enumerate(data):\n",
    "  label=2\n",
    "  y_neutral=np.append(y_neutral,label)\n",
    "x_neutral = np.append(x_neutral,data,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)\n",
    "print(x_right.shape, y_right.shape)\n",
    "print(x_left.shape, y_left.shape)\n",
    "print(x_neutral.shape, y_neutral.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train, Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_left = x_left[:,:,:,np.newaxis]\n",
    "x_right = x_right[:,:,:,np.newaxis]\n",
    "x_neutral = x_neutral[:,:,:,np.newaxis]\n",
    "split = 64\n",
    "\n",
    "x_train = np.concatenate((x_left[split:,:,:,:], x_right[split:,:,:,:], x_neutral[split:,:,:,:]), axis=0)\n",
    "x_val = np.concatenate((x_left[:split,:,:,:], x_right[:split,:,:,:], x_neutral[split:,:,:,:]), axis=0)\n",
    "\n",
    "y_train = np.concatenate((y_left[split:], y_right[split:], y_neutral[split:]))\n",
    "y_val = np.concatenate((y_left[:split], y_right[:split], y_neutral[split:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argwhere(np.isnan(x_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "files1 = glob(\"main/record/Test/*gauche*\")                                           \n",
    "files2 = glob(\"main/record/Test/*droite*\")                                           \n",
    "files3 = glob(\"main/record/Test/*neutre*\")\n",
    "\n",
    "dfs1 = []\n",
    "dfs2 = []\n",
    "dfs3 = []\n",
    "\n",
    "for f in files1:\n",
    "    df = pd.read_csv(f)\n",
    "    cols_remove = ['timestamps', 'Right AUX']\n",
    "    df = df.loc[:, ~df.columns.isin(cols_remove)]\n",
    "    df.columns = df.columns.str.replace('RAW_', '', 1)\n",
    "    df = df.fillna(df.mean())\n",
    "    dfs1.append(df)  \n",
    "\n",
    "data1_test = pd.concat(dfs1, ignore_index=True)\n",
    "# print(data1)\n",
    "\n",
    "for f in files2:\n",
    "    df = pd.read_csv(f)\n",
    "    cols_remove = ['timestamps', 'Right AUX']\n",
    "    df = df.loc[:, ~df.columns.isin(cols_remove)]\n",
    "    df.columns = df.columns.str.replace('RAW_', '', 1)\n",
    "    df = df.fillna(df.mean())\n",
    "    dfs2.append(df)  \n",
    "\n",
    "data2_test = pd.concat(dfs2, ignore_index=True)\n",
    "\n",
    "for f in files3:\n",
    "    df = pd.read_csv(f)\n",
    "    cols_remove = ['timestamps', 'Right AUX']\n",
    "    df = df.loc[:, ~df.columns.isin(cols_remove)]\n",
    "    df.columns = df.columns.str.replace('RAW_', '', 1)\n",
    "    df = df.fillna(df.mean())\n",
    "    dfs3.append(df)  \n",
    "\n",
    "data3_test = pd.concat(dfs3, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data1)\n",
    "print(data1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.empty((0, n_channels, n_samples))\n",
    "y_test = np.empty(0)\n",
    "\n",
    "\n",
    "# Gauche\n",
    "data = convertDF2MNE(data1_test)\n",
    "for _ in enumerate(data):\n",
    "    label = 0\n",
    "    y_test = np.append(y_test, label)\n",
    "x_test = np.concatenate((x_test, data), axis=0)\n",
    "\n",
    "# Droite\n",
    "data = convertDF2MNE(data2_test)\n",
    "for _ in enumerate(data):\n",
    "    label = 1\n",
    "    y_test = np.append(y_test, label)\n",
    "x_test = np.concatenate((x_test, data), axis=0)\n",
    "\n",
    "# Neutre\n",
    "data = convertDF2MNE(data3_test)\n",
    "for _ in enumerate(data):\n",
    "    label = 2  # Correction de l'étiquette neutre\n",
    "    y_test = np.append(y_test, label)\n",
    "x_test = np.concatenate((x_test, data), axis=0)\n",
    "\n",
    "x_test = x_test[:, :, :, np.newaxis]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_left = np.empty((0, n_channels, n_samples))\n",
    "y_left = np.empty(0)\n",
    "\n",
    "\n",
    "x_right = np.empty((0, n_channels, n_samples))\n",
    "y_right = np.empty(0)\n",
    "\n",
    "x_neutral = np.empty((0, n_channels, n_samples))\n",
    "y_neutral = np.empty(0)\n",
    "\n",
    "\n",
    "\n",
    "data = convertDF2MNE(data1)\n",
    "for i in enumerate(data):\n",
    "  label=0\n",
    "  y_left=np.append(y_left,label)\n",
    "x_left = np.append(x_left,data,axis=0)\n",
    "\n",
    "data = convertDF2MNE(data2)\n",
    "for i in enumerate(data):\n",
    "  label=1\n",
    "  y_right=np.append(y_right,label)\n",
    "x_right = np.append(x_right,data,axis=0)\n",
    "\n",
    "\n",
    "data = convertDF2MNE(data3)\n",
    "for i in enumerate(data):\n",
    "  label=2\n",
    "  y_neutral=np.append(y_neutral,label)\n",
    "x_neutral = np.append(x_neutral,data,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)\n",
    "print(x_right.shape, y_right.shape)\n",
    "print(x_left.shape, y_left.shape)\n",
    "print(x_neutral.shape, y_neutral.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader(tf.keras.utils.Sequence):\n",
    "    def __init__(self, images, labels, batch_size=32, shuffle=True):\n",
    "        super().__init__()\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.key_array = np.arange(self.images.shape[0], dtype=np.uint32)\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.key_array)//self.batch_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        keys = self.key_array[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        x = np.asarray(self.images[keys], dtype=np.float32)\n",
    "        y = np.asarray(self.labels[keys], dtype=np.float32)\n",
    "        return x, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            self.key_array = np.random.permutation(self.key_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(images=x_train, labels=y_train, batch_size=32, shuffle=True)\n",
    "n_batches = len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "ce_loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "\n",
    "loss_train = np.zeros(shape=(epochs,), dtype=np.float32)\n",
    "acc_train = np.zeros(shape=(epochs,), dtype=np.float32)\n",
    "loss_val = np.zeros(shape=(epochs,))\n",
    "acc_val = np.zeros(shape=(epochs,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Hybrid_CNN_LSTM(Chans=n_channels,Samples=n_samples)\n",
    "model.compile(optimizer=optimizer,loss=ce_loss)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "  epoch_loss_avg = tf.keras.metrics.Mean() # Keeping track of the training loss\n",
    "  epoch_acc_avg = tf.keras.metrics.Mean() # Keeping track of the training accuracy\n",
    "\n",
    "  print('==== Epoch #{0:3d} ===='.format(epoch))\n",
    "\n",
    "  for batch in tqdm(range(n_batches)):\n",
    "    x, y = dataloader[batch]\n",
    "\n",
    "    with tf.GradientTape() as tape: # Forward pass\n",
    "      y_ = model(x, training=True)\n",
    "      loss = ce_loss(y_true=y, y_pred=y_)\n",
    "\n",
    "    grad = tape.gradient(loss, model.trainable_variables) # Backpropagation\n",
    "    optimizer.apply_gradients(zip(grad, model.trainable_variables)) # Update network weights\n",
    "\n",
    "    epoch_loss_avg(loss)\n",
    "    epoch_acc_avg(sklearn.metrics.accuracy_score(y_true=y, y_pred=np.argmax(y_, axis=-1)))\n",
    "    \n",
    "  dataloader.on_epoch_end()\n",
    "\n",
    "  loss_train[epoch] = epoch_loss_avg.result()\n",
    "  acc_train[epoch] = epoch_acc_avg.result()\n",
    "\n",
    "  print('---- Training ----')\n",
    "  print('Loss  =  {0:.3f}'.format(loss_train[epoch]))\n",
    "  print('Acc   =  {0:.3f}'.format(acc_train[epoch]))\n",
    "\n",
    "  y_ = model.predict(x_val) # Validation predictions\n",
    "  loss_val[epoch] = ce_loss(y_true=y_val, y_pred=y_).numpy()\n",
    "  acc_val[epoch] = sklearn.metrics.accuracy_score(y_true=y_val, y_pred=np.argmax(y_, axis=-1))\n",
    "\n",
    "  print('--- Validation ---')\n",
    "  print('Loss  =  {0:.3f}'.format(loss_val[epoch]))\n",
    "  print('Acc   =  {0:.3f}'.format(acc_val[epoch]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"main/model/model1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training \n",
    "acc= sum(acc_train)/len(acc_train)\n",
    "print(f'Training Accuracy : {acc}')\n",
    "\n",
    "# Validation\n",
    "acval = sum(acc_val)/ len(acc_val)\n",
    "print(f'Validation Accuracy : {acval}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Plotting\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(loss_train, label = 'Training Loss')\n",
    "plt.plot(loss_val, label= 'Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Training', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Accuracy Plotting\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(acc_train, label = 'Training Accuracy')\n",
    "plt.plot(acc_val, label= 'Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Training', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = tf.keras.models.load_model(\"main/model/model1.h5\")\n",
    "\n",
    "# Model Architecture \n",
    "test_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ = test_model.predict(x_test)  \n",
    "\n",
    "loss_val[0] = ce_loss(y_true=y_test, y_pred=y_).numpy()\n",
    "acc_val[0] = sklearn.metrics.accuracy_score(y_true=y_test, y_pred=np.argmax(y_, axis=-1))\n",
    "\n",
    "for i in range(x_test.shape[0]):\n",
    "    if y_[i][0] > y_[i][1] and y_[i][0] > y_[i][2]:  \n",
    "        print('Predicted value : 0 with accuracy = {0:.3f}'.format(y_[i][0]))\n",
    "    elif y_[i][1] > y_[i][0] and y_[i][1] > y_[i][2]:  \n",
    "        print('Predicted value : 1 with accuracy = {0:.3f}'.format(y_[i][1]))\n",
    "    else: \n",
    "        print('Predicted value : 2 with accuracy = {0:.3f}'.format(y_[i][2]))\n",
    "    print('Actual value = {}\\n'.format(y_test[i]))\n",
    "\n",
    "print('--- Test ---')\n",
    "print('Loss  =  {0:.3f}'.format(loss_val[0]))\n",
    "print('Acc   =  {0:.3f}'.format(acc_val[0]))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
